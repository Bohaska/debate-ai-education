Title: The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances
URL: https://arxiv.org/abs/2407.09975
Citation:
> [!info] Citation
> ## Nie et al. 2024
> 
> Nie, Allen, Yash Chandak, Miroslav Suzara, Malika Ali, Juliette Woodrow, Matt Peng, Mehran Sahami, Emma Brunskill, and Chris Piech. “The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances.” arXiv, April 25, 2024. [https://doi.org/10.48550/arXiv.2407.09975](https://doi.org/10.48550/arXiv.2407.09975).

# Abstract

Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of interface is readily available to students and teachers around the world, yet relatively little research has been done to assess the impact of such generic tools on student learning. Coding education is an interesting test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. We estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.

# Notes

We found that only 44.1% of the students in the experiment group took the diagnostic exam, compared to 48.5% of the students in the control group who took the exam

Surprisingly, the disengagement trend reversed for the students who are from the low HDI countries: the exam participation in the experiment group is notably higher at 42.3% compared to participation in the control group, at 27.5%

We estimate adopters have a 6.8 percentage point average treatment effect improvement in their exam scores due to using the provided LLM.

